#!/usr/bin/env python3
"""
Script para gerar plots individuais de an√°lise comparativa entre GPT-2 baseline e Qwen-MoE
"""
import json
import matplotlib.pyplot as plt
import numpy as np
import os
from pathlib import Path

# Configura√ß√µes de estilo
plt.style.use('seaborn-v0_8')
plt.rcParams['figure.figsize'] = (10, 6)
plt.rcParams['font.size'] = 12

def load_metrics(metrics_path):
    """Carrega m√©tricas de um arquivo JSON"""
    with open(metrics_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def plot_loss_comparison():
    """Plot comparativo de loss de treinamento"""
    baseline_metrics = load_metrics('results/metrics/baseline_gpt2_small_metrics.json')
    moe_metrics = load_metrics('results/metrics/qwen_moe_6experts_top1_metrics.json')
    
    plt.figure(figsize=(12, 6))
    
    steps_baseline = list(range(len(baseline_metrics['train_losses'])))
    steps_moe = list(range(len(moe_metrics['train_losses'])))
    
    plt.plot(steps_baseline, baseline_metrics['train_losses'], 
             label='GPT-2 Baseline', color='#1f77b4', linewidth=2)
    plt.plot(steps_moe, moe_metrics['train_losses'], 
             label='Qwen-MoE (6 experts)', color='#ff7f0e', linewidth=2)
    
    plt.xlabel('Passos de Avalia√ß√£o')
    plt.ylabel('Loss de Treinamento')
    plt.title('Comparativo de Converg√™ncia - Loss de Treinamento')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('results/plots/individual/loss_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Loss comparison: results/plots/individual/loss_comparison.png")

def plot_tokens_vs_loss():
    """Plot de loss vs tokens processados"""
    baseline_metrics = load_metrics('results/metrics/baseline_gpt2_small_metrics.json')
    moe_metrics = load_metrics('results/metrics/qwen_moe_6experts_top1_metrics.json')
    
    plt.figure(figsize=(12, 6))
    
    tokens_baseline = np.array(baseline_metrics['tokens_seen'])/1e6
    tokens_moe = np.array(moe_metrics['tokens_seen'])/1e6
    
    plt.plot(tokens_baseline, baseline_metrics['train_losses'], 
             label='GPT-2 Baseline', color='#1f77b4', linewidth=2)
    plt.plot(tokens_moe, moe_metrics['train_losses'], 
             label='Qwen-MoE (6 experts)', color='#ff7f0e', linewidth=2)
    
    plt.xlabel('Tokens Processados (Milh√µes)')
    plt.ylabel('Loss de Treinamento')
    plt.title('Loss vs Tokens Processados')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('results/plots/individual/tokens_vs_loss.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Tokens vs Loss: results/plots/individual/tokens_vs_loss.png")

def plot_perplexity_comparison():
    """Plot comparativo de perplexidade final"""
    baseline_metrics = load_metrics('results/metrics/baseline_gpt2_small_metrics.json')
    moe_metrics = load_metrics('results/metrics/qwen_moe_6experts_top1_metrics.json')
    
    plt.figure(figsize=(8, 6))
    
    models = ['GPT-2\nBaseline', 'Qwen-MoE\n(6 experts)']
    perplexities = [baseline_metrics['test_ppl'], moe_metrics['test_ppl']]
    colors = ['#1f77b4', '#ff7f0e']
    
    bars = plt.bar(models, perplexities, color=colors, alpha=0.7, width=0.6)
    plt.ylabel('Perplexidade de Teste')
    plt.title('Comparativo de Perplexidade Final')
    plt.grid(True, alpha=0.3, axis='y')
    
    # Adicionar valores nas barras
    for bar, ppl in zip(bars, perplexities):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                f'{ppl:.1f}', ha='center', va='bottom', fontweight='bold', fontsize=14)
    
    plt.tight_layout()
    plt.savefig('results/plots/individual/perplexity_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Perplexity comparison: results/plots/individual/perplexity_comparison.png")

def plot_throughput_comparison():
    """Plot comparativo de throughput"""
    baseline_metrics = load_metrics('results/metrics/baseline_gpt2_small_metrics.json')
    moe_metrics = load_metrics('results/metrics/qwen_moe_6experts_top1_metrics.json')
    
    plt.figure(figsize=(8, 6))
    
    models = ['GPT-2\nBaseline', 'Qwen-MoE\n(6 experts)']
    throughputs = [baseline_metrics['avg_tps'], moe_metrics['avg_tps']]
    colors = ['#1f77b4', '#ff7f0e']
    
    bars = plt.bar(models, throughputs, color=colors, alpha=0.7, width=0.6)
    plt.ylabel('Tokens por Segundo')
    plt.title('Comparativo de Throughput de Treinamento')
    plt.grid(True, alpha=0.3, axis='y')
    
    # Adicionar valores nas barras
    for bar, tps in zip(bars, throughputs):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1000,
                f'{tps:,.0f}', ha='center', va='bottom', fontweight='bold', fontsize=12)
    
    plt.tight_layout()
    plt.savefig('results/plots/individual/throughput_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Throughput comparison: results/plots/individual/throughput_comparison.png")

def plot_memory_usage():
    """Plot comparativo de uso de mem√≥ria"""
    baseline_metrics = load_metrics('results/metrics/baseline_gpt2_small_metrics.json')
    moe_metrics = load_metrics('results/metrics/qwen_moe_6experts_top1_metrics.json')
    
    plt.figure(figsize=(8, 6))
    
    models = ['GPT-2\nBaseline', 'Qwen-MoE\n(6 experts)']
    memory_usage = [baseline_metrics['peak_mem_gb'], moe_metrics['peak_mem_gb']]
    colors = ['#1f77b4', '#ff7f0e']
    
    bars = plt.bar(models, memory_usage, color=colors, alpha=0.7, width=0.6)
    plt.ylabel('Mem√≥ria de Pico (GB)')
    plt.title('Comparativo de Consumo de Mem√≥ria')
    plt.grid(True, alpha=0.3, axis='y')
    
    # Adicionar valores nas barras
    for bar, mem in zip(bars, memory_usage):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                f'{mem:.1f} GB', ha='center', va='bottom', fontweight='bold', fontsize=14)
    
    plt.tight_layout()
    plt.savefig('results/plots/individual/memory_usage.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Memory usage: results/plots/individual/memory_usage.png")

def plot_efficiency():
    """Plot de efici√™ncia computacional (tokens/seg por GB)"""
    baseline_metrics = load_metrics('results/metrics/baseline_gpt2_small_metrics.json')
    moe_metrics = load_metrics('results/metrics/qwen_moe_6experts_top1_metrics.json')
    
    plt.figure(figsize=(8, 6))
    
    models = ['GPT-2\nBaseline', 'Qwen-MoE\n(6 experts)']
    throughputs = [baseline_metrics['avg_tps'], moe_metrics['avg_tps']]
    memory_usage = [baseline_metrics['peak_mem_gb'], moe_metrics['peak_mem_gb']]
    efficiency = [t/m for t, m in zip(throughputs, memory_usage)]
    colors = ['#1f77b4', '#ff7f0e']
    
    bars = plt.bar(models, efficiency, color=colors, alpha=0.7, width=0.6)
    plt.ylabel('Tokens/seg por GB de Mem√≥ria')
    plt.title('Efici√™ncia Computacional')
    plt.grid(True, alpha=0.3, axis='y')
    
    # Adicionar valores nas barras
    for bar, eff in zip(bars, efficiency):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1000,
                f'{eff:,.0f}', ha='center', va='bottom', fontweight='bold', fontsize=12)
    
    plt.tight_layout()
    plt.savefig('results/plots/individual/computational_efficiency.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Computational efficiency: results/plots/individual/computational_efficiency.png")

def plot_smoothed_learning_curve():
    """Plot de curva de aprendizado suavizada"""
    baseline_metrics = load_metrics('results/metrics/baseline_gpt2_small_metrics.json')
    moe_metrics = load_metrics('results/metrics/qwen_moe_6experts_top1_metrics.json')
    
    plt.figure(figsize=(12, 6))
    
    # Suavizar curvas com m√©dia m√≥vel
    def smooth_curve(y, window=10):
        return np.convolve(y, np.ones(window)/window, mode='valid')
    
    tokens_baseline = np.array(baseline_metrics['tokens_seen'])/1e6
    tokens_moe = np.array(moe_metrics['tokens_seen'])/1e6
    
    losses_baseline_smooth = smooth_curve(baseline_metrics['train_losses'])
    losses_moe_smooth = smooth_curve(moe_metrics['train_losses'])
    
    tokens_baseline_smooth = tokens_baseline[9:]  # Ajustar para o tamanho da suaviza√ß√£o
    tokens_moe_smooth = tokens_moe[9:]
    
    # Curvas suavizadas
    plt.plot(tokens_baseline_smooth, losses_baseline_smooth, 
             label='GPT-2 Baseline (suavizado)', color='#1f77b4', linewidth=3)
    plt.plot(tokens_moe_smooth, losses_moe_smooth, 
             label='Qwen-MoE (suavizado)', color='#ff7f0e', linewidth=3)
    
    # Pontos originais mais transparentes
    plt.plot(tokens_baseline, baseline_metrics['train_losses'], 
             color='#1f77b4', alpha=0.2, linewidth=1, label='GPT-2 (original)')
    plt.plot(tokens_moe, moe_metrics['train_losses'], 
             color='#ff7f0e', alpha=0.2, linewidth=1, label='Qwen-MoE (original)')
    
    plt.xlabel('Tokens Processados (Milh√µes)')
    plt.ylabel('Loss de Treinamento')
    plt.title('Curva de Aprendizado Suavizada')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('results/plots/individual/smoothed_learning_curve.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Smoothed learning curve: results/plots/individual/smoothed_learning_curve.png")

def plot_improvement_rate():
    """Plot de taxa de melhoria por token"""
    baseline_metrics = load_metrics('results/metrics/baseline_gpt2_small_metrics.json')
    moe_metrics = load_metrics('results/metrics/qwen_moe_6experts_top1_metrics.json')
    
    plt.figure(figsize=(12, 6))
    
    # Calcular derivada (taxa de melhoria)
    def compute_improvement_rate(tokens, losses, window=5):
        rates = []
        for i in range(window, len(losses)):
            # Taxa de melhoria: -delta_loss / delta_tokens
            delta_loss = losses[i] - losses[i-window]
            delta_tokens = tokens[i] - tokens[i-window]
            if delta_tokens > 0:
                rates.append(-delta_loss / delta_tokens * 1e6)  # Por milh√£o de tokens
            else:
                rates.append(0)
        return rates
    
    tokens_baseline = np.array(baseline_metrics['tokens_seen'])/1e6
    tokens_moe = np.array(moe_metrics['tokens_seen'])/1e6
    
    rates_baseline = compute_improvement_rate(tokens_baseline, baseline_metrics['train_losses'])
    rates_moe = compute_improvement_rate(tokens_moe, moe_metrics['train_losses'])
    
    tokens_rates_baseline = tokens_baseline[5:]
    tokens_rates_moe = tokens_moe[5:]
    
    plt.plot(tokens_rates_baseline, rates_baseline, 
             label='GPT-2 Baseline', color='#1f77b4', linewidth=2)
    plt.plot(tokens_rates_moe, rates_moe, 
             label='Qwen-MoE (6 experts)', color='#ff7f0e', linewidth=2)
    
    plt.xlabel('Tokens Processados (Milh√µes)')
    plt.ylabel('Taxa de Melhoria (Œîloss/Mtokens)')
    plt.title('Taxa de Melhoria por Token')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)
    
    plt.tight_layout()
    plt.savefig('results/plots/individual/improvement_rate.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Improvement rate: results/plots/individual/improvement_rate.png")

def plot_training_variability():
    """Plot de variabilidade do treinamento"""
    baseline_metrics = load_metrics('results/metrics/baseline_gpt2_small_metrics.json')
    moe_metrics = load_metrics('results/metrics/qwen_moe_6experts_top1_metrics.json')
    
    plt.figure(figsize=(12, 6))
    
    # Variabilidade do loss (desvio padr√£o m√≥vel)
    def rolling_std(data, window=10):
        return [np.std(data[max(0, i-window):i+1]) for i in range(len(data))]
    
    std_baseline = rolling_std(baseline_metrics['train_losses'])
    std_moe = rolling_std(moe_metrics['train_losses'])
    
    steps_baseline = list(range(len(std_baseline)))
    steps_moe = list(range(len(std_moe)))
    
    plt.plot(steps_baseline, std_baseline, label='GPT-2 Baseline', color='#1f77b4', linewidth=2)
    plt.plot(steps_moe, std_moe, label='Qwen-MoE (6 experts)', color='#ff7f0e', linewidth=2)
    plt.xlabel('Passos de Avalia√ß√£o')
    plt.ylabel('Desvio Padr√£o do Loss (Janela M√≥vel)')
    plt.title('Variabilidade do Treinamento')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('results/plots/individual/training_variability.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Training variability: results/plots/individual/training_variability.png")

def plot_loss_distribution():
    """Plot de distribui√ß√£o dos valores de loss"""
    baseline_metrics = load_metrics('results/metrics/baseline_gpt2_small_metrics.json')
    moe_metrics = load_metrics('results/metrics/qwen_moe_6experts_top1_metrics.json')
    
    plt.figure(figsize=(10, 6))
    
    plt.hist(baseline_metrics['train_losses'], bins=30, alpha=0.7, 
             label='GPT-2 Baseline', color='#1f77b4', density=True)
    plt.hist(moe_metrics['train_losses'], bins=30, alpha=0.7, 
             label='Qwen-MoE (6 experts)', color='#ff7f0e', density=True)
    
    plt.xlabel('Valor do Loss')
    plt.ylabel('Densidade')
    plt.title('Distribui√ß√£o dos Valores de Loss Durante o Treinamento')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('results/plots/individual/loss_distribution.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Loss distribution: results/plots/individual/loss_distribution.png")

def plot_loss_boxplot():
    """Box plot comparativo dos valores de loss"""
    baseline_metrics = load_metrics('results/metrics/baseline_gpt2_small_metrics.json')
    moe_metrics = load_metrics('results/metrics/qwen_moe_6experts_top1_metrics.json')
    
    plt.figure(figsize=(8, 6))
    
    data_to_plot = [baseline_metrics['train_losses'], moe_metrics['train_losses']]
    box_plot = plt.boxplot(data_to_plot, tick_labels=['GPT-2\nBaseline', 'Qwen-MoE\n(6 experts)'], 
                          patch_artist=True)
    
    colors = ['#1f77b4', '#ff7f0e']
    for patch, color in zip(box_plot['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)
    
    plt.ylabel('Loss de Treinamento')
    plt.title('Distribui√ß√£o Comparativa do Loss')
    plt.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig('results/plots/individual/loss_boxplot.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Loss boxplot: results/plots/individual/loss_boxplot.png")

def plot_moe_parameters():
    """Plot dos par√¢metros espec√≠ficos do modelo MoE"""
    moe_metrics = load_metrics('results/metrics/qwen_moe_6experts_top1_metrics.json')
    
    plt.figure(figsize=(10, 6))
    
    if 'num_experts' in moe_metrics:
        moe_chars = ['N√∫mero de\nExperts', 'Top-K', 'Capacity\nFactor', 'Aux Loss\nWeight']
        moe_values = [moe_metrics.get('num_experts', 0), 
                     moe_metrics.get('top_k', 0),
                     moe_metrics.get('capacity_factor', 0),
                     moe_metrics.get('aux_loss_weight', 0)]
        
        bars = plt.bar(moe_chars, moe_values, color='#ff7f0e', alpha=0.7, width=0.6)
        plt.ylabel('Valores')
        plt.title('Par√¢metros do Modelo Mixture of Experts (MoE)')
        plt.grid(True, alpha=0.3, axis='y')
        
        for bar, val in zip(bars, moe_values):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(moe_values)*0.02,
                    f'{val}', ha='center', va='bottom', fontweight='bold', fontsize=12)
    
    plt.tight_layout()
    plt.savefig('results/plots/individual/moe_parameters.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì MoE parameters: results/plots/individual/moe_parameters.png")

def plot_performance_radar():
    """Radar chart de performance normalizada"""
    baseline_metrics = load_metrics('results/metrics/baseline_gpt2_small_metrics.json')
    moe_metrics = load_metrics('results/metrics/qwen_moe_6experts_top1_metrics.json')
    
    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
    
    # M√©tricas normalizadas para compara√ß√£o
    metrics = {
        'Perplexidade\n(menor melhor)': {
            'baseline': baseline_metrics['test_ppl'],
            'moe': moe_metrics['test_ppl'],
            'invert': True  # Menor √© melhor
        },
        'Throughput\n(maior melhor)': {
            'baseline': baseline_metrics['avg_tps'],
            'moe': moe_metrics['avg_tps'],
            'invert': False
        },
        'Efici√™ncia Mem√≥ria\n(maior melhor)': {
            'baseline': baseline_metrics['avg_tps'] / baseline_metrics['peak_mem_gb'],
            'moe': moe_metrics['avg_tps'] / moe_metrics['peak_mem_gb'],
            'invert': False
        },
        'Loss Final\n(menor melhor)': {
            'baseline': baseline_metrics['train_losses'][-1],
            'moe': moe_metrics['train_losses'][-1],
            'invert': True
        },
        'Estabilidade\n(menor melhor)': {
            'baseline': np.std(baseline_metrics['train_losses'][-20:]),  # √öltimos 20 pontos
            'moe': np.std(moe_metrics['train_losses'][-20:]),
            'invert': True
        }
    }
    
    # Normalizar m√©tricas (0-1 scale)
    normalized_metrics = {}
    for metric, values in metrics.items():
        baseline_val = values['baseline']
        moe_val = values['moe']
        
        if values['invert']:
            # Para m√©tricas onde menor √© melhor, invertemos
            max_val = max(baseline_val, moe_val)
            normalized_baseline = 1 - (baseline_val / max_val)
            normalized_moe = 1 - (moe_val / max_val)
        else:
            # Para m√©tricas onde maior √© melhor
            max_val = max(baseline_val, moe_val)
            normalized_baseline = baseline_val / max_val
            normalized_moe = moe_val / max_val
        
        normalized_metrics[metric] = {
            'baseline': normalized_baseline,
            'moe': normalized_moe
        }
    
    # Criar radar chart
    labels = list(normalized_metrics.keys())
    baseline_scores = [normalized_metrics[label]['baseline'] for label in labels]
    moe_scores = [normalized_metrics[label]['moe'] for label in labels]
    
    # √Çngulos para o radar chart
    angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()
    baseline_scores += baseline_scores[:1]  # Fechar o c√≠rculo
    moe_scores += moe_scores[:1]
    angles += angles[:1]
    
    ax.plot(angles, baseline_scores, 'o-', linewidth=3, label='GPT-2 Baseline', color='#1f77b4')
    ax.fill(angles, baseline_scores, alpha=0.25, color='#1f77b4')
    ax.plot(angles, moe_scores, 'o-', linewidth=3, label='Qwen-MoE', color='#ff7f0e')
    ax.fill(angles, moe_scores, alpha=0.25, color='#ff7f0e')
    
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(labels)
    ax.set_ylim(0, 1)
    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
    ax.set_yticklabels(['20%', '40%', '60%', '80%', '100%'])
    ax.grid(True)
    
    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))
    plt.title('Comparativo Geral de Performance\n(M√©tricas Normalizadas)', size=16, pad=30)
    
    plt.tight_layout()
    plt.savefig('results/plots/individual/performance_radar.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("‚úì Performance radar: results/plots/individual/performance_radar.png")

def main():
    """Fun√ß√£o principal para gerar todos os plots individuais"""
    print("üé® Gerando plots individuais de an√°lise comparativa...")
    print("=" * 60)
    
    # Criar diret√≥rio de plots individuais se n√£o existir
    os.makedirs('results/plots/individual', exist_ok=True)
    
    # Gerar todos os plots individuais
    plot_loss_comparison()
    plot_tokens_vs_loss()
    plot_perplexity_comparison()
    plot_throughput_comparison()
    plot_memory_usage()
    plot_efficiency()
    plot_smoothed_learning_curve()
    plot_improvement_rate()
    plot_training_variability()
    plot_loss_distribution()
    plot_loss_boxplot()
    plot_moe_parameters()
    plot_performance_radar()
    
    print("=" * 60)
    print("‚úÖ Todos os plots individuais foram gerados com sucesso!")
    print("\nüìÅ Pasta: results/plots/individual/")
    print("üîç Plots de compara√ß√£o:")
    print("   ‚Ä¢ loss_comparison.png - Comparativo de converg√™ncia")
    print("   ‚Ä¢ tokens_vs_loss.png - Loss vs tokens processados")
    print("   ‚Ä¢ perplexity_comparison.png - Perplexidade final")
    print("   ‚Ä¢ throughput_comparison.png - Velocidade de processamento")
    print("   ‚Ä¢ memory_usage.png - Consumo de mem√≥ria")
    print("   ‚Ä¢ computational_efficiency.png - Efici√™ncia computacional")
    print("\nüìà Plots de an√°lise de treinamento:")
    print("   ‚Ä¢ smoothed_learning_curve.png - Curva de aprendizado suavizada")
    print("   ‚Ä¢ improvement_rate.png - Taxa de melhoria por token")
    print("   ‚Ä¢ training_variability.png - Estabilidade do treinamento")
    print("   ‚Ä¢ loss_distribution.png - Distribui√ß√£o dos valores de loss")
    print("   ‚Ä¢ loss_boxplot.png - Box plot comparativo")
    print("\nüîß Plots espec√≠ficos:")
    print("   ‚Ä¢ moe_parameters.png - Par√¢metros do modelo MoE")
    print("   ‚Ä¢ performance_radar.png - Radar chart de performance")

if __name__ == "__main__":
    main()